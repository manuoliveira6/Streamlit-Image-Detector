{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import wandb\n",
    "import torchvision\n",
    "from cnn import CNN, load_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "\n",
    "classification_models = torchvision.models.list_models(module=torchvision.models)\n",
    "\n",
    "def validate_model(model_name):\n",
    "    if model_name not in classification_models:\n",
    "        raise ValueError(f\"El modelo {model_name} no está en la lista de modelos de clasificación disponibles.\")\n",
    "\n",
    "def main(model, learning_rate, epochs):\n",
    "    # Validar el modelo\n",
    "    validate_model(model)\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"Streamlit\",\n",
    "        config={\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"architecture\": \"my_model\",\n",
    "            \"dataset\": \"my_dataset\",\n",
    "            \"epochs\": epochs,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    train_dir = '../dataset/training'\n",
    "    valid_dir = '../dataset/validation'\n",
    "\n",
    "    train_loader, valid_loader, num_classes = load_data(train_dir, \n",
    "                                                    valid_dir, \n",
    "                                                    batch_size=32, \n",
    "                                                    img_size=224) \n",
    "\n",
    "    my_model = CNN(getattr(torchvision.models, model)(weights='DEFAULT'), num_classes)\n",
    "    optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print('Empezando el entrenamiento...')\n",
    "    for epoch in range(epochs):\n",
    "        # Entrenamiento\n",
    "        my_model.train()\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = my_model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_preds.extend(torch.argmax(output, 1).tolist())\n",
    "            train_targets.extend(labels.tolist())\n",
    "\n",
    "        # Validación\n",
    "        my_model.eval()\n",
    "        valid_losses = []\n",
    "        valid_preds = []\n",
    "        valid_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                output = my_model(images)\n",
    "                loss = criterion(output, labels)\n",
    "\n",
    "                valid_losses.append(loss.item())\n",
    "                valid_preds.extend(torch.argmax(output, 1).tolist())\n",
    "                valid_targets.extend(labels.tolist())\n",
    "\n",
    "        # Calcular métricas\n",
    "        train_accuracy = accuracy_score(train_targets, train_preds)\n",
    "        valid_accuracy = accuracy_score(valid_targets, valid_preds)\n",
    "\n",
    "        # Registrar métricas en W&B\n",
    "        wandb.log({\"train_loss\": sum(train_losses) / len(train_losses),\n",
    "                   \"train_accuracy\": train_accuracy,\n",
    "                   \"valid_loss\": sum(valid_losses) / len(valid_losses),\n",
    "                   \"valid_accuracy\": valid_accuracy})\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}: Train Loss: {sum(train_losses) / len(train_losses)}, Train Accuracy: {train_accuracy}, Valid Loss: {sum(valid_losses) / len(valid_losses)}, Valid Accuracy: {valid_accuracy}\")\n",
    "\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    my_model.save(f'{model}-{epochs}epoch-{learning_rate}lr')\n",
    "\n",
    "\n",
    "# Configurar y parsear los argumentos de línea de comandos\n",
    "parser = argparse.ArgumentParser(description=\"Script para entrenar modelos de clasificación.\")\n",
    "parser.add_argument(\"--model\", type=str, default='resnet50', nargs='?', help=\"Nombre del modelo de clasificación (ej. ResNet50)\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.001, nargs='?', help=\"Tasa de aprendizaje (default: 0.001)\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=10, nargs='?', help=\"Número de epochs (default: 10)\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(args.model, args.learning_rate, args.epochs)\n",
    "    # python weightandbiases.py --model resnet50 --learning_rate 0.001 --epochs 10\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
